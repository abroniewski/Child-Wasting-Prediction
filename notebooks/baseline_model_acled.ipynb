{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''=============================================================\n",
    "===================== SECTION IMPORTS ==========================\n",
    "================================================================'''\n",
    "from time import time\n",
    "\n",
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.base\n",
    "from numpy import logspace, linspace\n",
    "from pandas import DataFrame\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso, ElasticNetCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_validate, RandomizedSearchCV\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the path to the dataset folder\n",
    "your_datapath = '../data/ZHL/'\n",
    "#district_name = \"Afgooye\" #Adan Yabaal, Afgooye, Afmadow\n",
    "\n",
    "# new features data path\n",
    "acled_datapath = '../data/acled/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space for number of trees in random forest and depth of trees\n",
    "num_trees_min = 64\n",
    "num_trees_max = 128\n",
    "\n",
    "depth_min = 2\n",
    "depth_max = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates a pandas dataframe for a single district with columns for the baseline model with semi-yearly entries\n",
    "def make_district_df_semiyearly(datapath, acled_datapath, district_name):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that creates a pandas dataframe for a single district with columns for the baseline model with semiyearly entries\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapath : string\n",
    "        Path to the datafolder\n",
    "    district_name : string\n",
    "        Name of the district\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Read all relevant datasets\n",
    "    prevalence_df = pd.read_csv(datapath + 'prevalence_estimates.csv', parse_dates=['date'])\n",
    "    covid_df = pd.read_csv(datapath + 'covid.csv', parse_dates=['date'])\n",
    "    ipc_df = pd.read_csv(datapath + 'ipc2.csv', parse_dates=['date'])\n",
    "    risk_df = pd.read_csv(datapath + 'FSNAU_riskfactors.csv', parse_dates=['date'])\n",
    "    production_df = pd.read_csv(datapath + 'production.csv', parse_dates=['date'])\n",
    "    # # updated code dc3\n",
    "    # conflict_df = pd.read_csv(acled_datapath + 'acled_features.csv', parse_dates=['date'])\n",
    "\n",
    "\n",
    "    # Select data for specific district\n",
    "    prevalence_df = prevalence_df[prevalence_df['district'] == district_name]\n",
    "    ipc_df = ipc_df[ipc_df['district'] == district_name]\n",
    "    risk_df = risk_df[risk_df['district'] == district_name]\n",
    "    production_df = production_df[production_df['district'] == district_name]\n",
    "    # # updated code dc3\n",
    "    # conflict_df = conflict_df[conflict_df['district'] == district_name].copy()\n",
    "    # conflict_df.drop(columns=['district'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    risk_df = risk_df.groupby(pd.Grouper(key='date', freq='6M')).mean()\n",
    "    risk_df = risk_df.reset_index()\n",
    "    risk_df['date'] = risk_df['date'].apply(lambda x: x.replace(day=1))\n",
    "\n",
    "    covid_df = covid_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "    covid_df = covid_df.reset_index()\n",
    "    covid_df['date'] = covid_df['date'].apply(lambda x: x.replace(day=1))\n",
    "\n",
    "    production_df['cropdiv'] = production_df.count(axis=1)\n",
    "\n",
    "    # Sort dataframes on date\n",
    "    prevalence_df.sort_values('date', inplace=True)\n",
    "    covid_df.sort_values('date', inplace=True)\n",
    "    ipc_df.sort_values('date', inplace=True)\n",
    "    risk_df.sort_values('date', inplace=True)\n",
    "    production_df.sort_values('date', inplace=True)\n",
    "    # # updated code dc3\n",
    "    # conflict_df.sort_values('date', inplace=True)\n",
    "\n",
    "\n",
    "    # Merge dataframes, only joining on current or previous dates as to prevent data leakage\n",
    "    df = pd.merge_asof(left=prevalence_df, right=ipc_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=production_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=risk_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=covid_df, direction='backward', on='date')\n",
    "    # # updated code dc3 \n",
    "    # df = pd.merge_asof(left=df, right=conflict_df, direction='backward', on='date')\n",
    "\n",
    "\n",
    "    # Calculate prevalence 6lag\n",
    "    df['prevalence_6lag'] = df['GAM Prevalence'].shift(1)\n",
    "    df['next_prevalence'] = df['GAM Prevalence'].shift(-1)\n",
    "\n",
    "    # Select needed columns\n",
    "    df = df[['date', 'district_x', 'GAM Prevalence', 'next_prevalence', 'prevalence_6lag', 'new_cases', 'ndvi_score',\n",
    "             'phase3plus_perc', 'cropdiv', 'total population']]\n",
    "    df.columns = ['date', 'district', 'prevalence', 'next_prevalence', 'prevalence_6lag', 'covid', 'ndvi', 'ipc',\n",
    "                  'cropdiv', 'population']\n",
    "\n",
    "    # # updated code dc3\n",
    "    # conflict_cols = conflict_df.columns.to_list()[2:]\n",
    "    # df = df[['date', 'district_x', 'GAM Prevalence', 'next_prevalence', 'prevalence_6lag', 'new_cases', 'ndvi_score',\n",
    "    #          'phase3plus_perc', 'cropdiv', 'total population'] + conflict_cols ]\n",
    "    # df.columns = ['date', 'district', 'prevalence', 'next_prevalence', 'prevalence_6lag', 'covid', 'ndvi', 'ipc',\n",
    "    #               'cropdiv', 'population'] + conflict_cols\n",
    "\n",
    "\n",
    "    # Add month column\n",
    "    df['month'] = df['date'].dt.month\n",
    "\n",
    "    # Add target variable: increase for next month prevalence (boolean)\n",
    "    increase = [False if x[1] < x[0] else True for x in list(zip(df['prevalence'], df['prevalence'][1:]))]\n",
    "    increase.append(False)\n",
    "    df['increase'] = increase\n",
    "    df.iloc[-1, df.columns.get_loc('increase')] = np.nan  # No info on next month\n",
    "\n",
    "    # Add target variable: increase for next month prevalence (boolean)\n",
    "    increase_numeric = [x[1] - x[0] for x in list(zip(df['prevalence'], df['prevalence'][1:]))]\n",
    "    increase_numeric.append(0)\n",
    "    df['increase_numeric'] = increase_numeric\n",
    "    df.iloc[-1, df.columns.get_loc('increase_numeric')] = np.nan  # No info on next month\n",
    "\n",
    "    df.loc[(df.date < pd.to_datetime('2020-03-01')), 'covid'] = 0\n",
    "\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that combines the semiyearly dataset (from the function make_district_df_semiyearly) of all districts\n",
    "def make_combined_df_semiyearly(datapath,acled_datapath):\n",
    "    \"\"\"\n",
    "    Function that creates a pandas dataframe for all districts with columns for the baseline model with semiyearly entries\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapath : string\n",
    "        Path to the datafolder\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    prevdf = pd.read_csv(datapath + 'prevalence_estimates.csv', parse_dates=['date'])\n",
    "    districts = prevdf['district'].unique()\n",
    "\n",
    "    df_list = []\n",
    "    for district in districts:\n",
    "        district_df = make_district_df_semiyearly(datapath, acled_datapath, district)\n",
    "        district_df['district'] = district\n",
    "        df_list.append(district_df)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df['district_encoded'] = df['district'].astype('category').cat.codes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns every possible subset (except the empty set) of the input list l\n",
    "def subsets(l):\n",
    "    subset_list = []\n",
    "    for i in range(len(l) + 1):\n",
    "        for j in range(i):\n",
    "            subset_list.append(l[j: i])\n",
    "    return subset_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================================================================================================\n",
    "## MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>district</th>\n",
       "      <th>prevalence</th>\n",
       "      <th>next_prevalence</th>\n",
       "      <th>prevalence_6lag</th>\n",
       "      <th>covid</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>ipc</th>\n",
       "      <th>cropdiv</th>\n",
       "      <th>population</th>\n",
       "      <th>month</th>\n",
       "      <th>increase</th>\n",
       "      <th>increase_numeric</th>\n",
       "      <th>district_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>Adan Yabaal</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.526296e+04</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.018200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Adan Yabaal</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.278939</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.526296e+04</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.072061</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>Adan Yabaal</td>\n",
       "      <td>0.278939</td>\n",
       "      <td>0.395974</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.785915e+04</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>0.117034</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Adan Yabaal</td>\n",
       "      <td>0.395974</td>\n",
       "      <td>0.340623</td>\n",
       "      <td>0.278939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.273333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.785915e+04</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.055351</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>Adan Yabaal</td>\n",
       "      <td>0.340623</td>\n",
       "      <td>0.312611</td>\n",
       "      <td>0.395974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.785915e+04</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.028011</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>Ceel barde</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.607096e+04</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Ceel barde</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.607096e+04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>Mogadishu</td>\n",
       "      <td>0.389306</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.077274e+06</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.098106</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Mogadishu</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.389306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.707985e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>Burao</td>\n",
       "      <td>0.335941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.606449e+05</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>677 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date     district  prevalence  next_prevalence  prevalence_6lag  \\\n",
       "0   2017-07-01  Adan Yabaal    0.369200         0.351000              NaN   \n",
       "1   2018-01-01  Adan Yabaal    0.351000         0.278939         0.369200   \n",
       "2   2018-07-01  Adan Yabaal    0.278939         0.395974         0.351000   \n",
       "3   2019-01-01  Adan Yabaal    0.395974         0.340623         0.278939   \n",
       "4   2019-07-01  Adan Yabaal    0.340623         0.312611         0.395974   \n",
       "..         ...          ...         ...              ...              ...   \n",
       "672 2017-07-01   Ceel barde    0.486200         0.486200              NaN   \n",
       "673 2018-01-01   Ceel barde    0.486200              NaN         0.486200   \n",
       "674 2017-07-01    Mogadishu    0.389306         0.291200              NaN   \n",
       "675 2018-01-01    Mogadishu    0.291200              NaN         0.389306   \n",
       "676 2017-07-01        Burao    0.335941              NaN              NaN   \n",
       "\n",
       "     covid      ndvi  ipc  cropdiv    population  month increase  \\\n",
       "0      0.0  0.215000  NaN      2.0  6.526296e+04      7    False   \n",
       "1      0.0  0.225000  NaN      4.0  6.526296e+04      1    False   \n",
       "2      0.0  0.260000  NaN      4.0  3.785915e+04      7     True   \n",
       "3      0.0  0.273333  NaN      4.0  3.785915e+04      1    False   \n",
       "4      0.0  0.246667  NaN      3.0  3.785915e+04      7    False   \n",
       "..     ...       ...  ...      ...           ...    ...      ...   \n",
       "672    0.0       NaN  NaN      NaN  4.607096e+04      7     True   \n",
       "673    0.0       NaN  NaN      NaN  4.607096e+04      1      NaN   \n",
       "674    0.0       NaN  NaN      NaN  2.077274e+06      7    False   \n",
       "675    0.0       NaN  NaN      NaN  1.707985e+06      1      NaN   \n",
       "676    0.0       NaN  NaN      NaN  5.606449e+05      7      NaN   \n",
       "\n",
       "     increase_numeric  district_encoded  \n",
       "0           -0.018200                 2  \n",
       "1           -0.072061                 2  \n",
       "2            0.117034                 2  \n",
       "3           -0.055351                 2  \n",
       "4           -0.028011                 2  \n",
       "..                ...               ...  \n",
       "672          0.000000                41  \n",
       "673               NaN                41  \n",
       "674         -0.098106                68  \n",
       "675               NaN                68  \n",
       "676               NaN                26  \n",
       "\n",
       "[677 rows x 14 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe for all districts\n",
    "df = make_combined_df_semiyearly(your_datapath,acled_datapath)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of district before droping are -  87\n"
     ]
    }
   ],
   "source": [
    "print(\"Total no of district before droping are - \",len(df['district'].value_counts().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prevalence</th>\n",
       "      <td>677.0</td>\n",
       "      <td>0.376678</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.091189</td>\n",
       "      <td>0.312611</td>\n",
       "      <td>0.384560</td>\n",
       "      <td>0.445262</td>\n",
       "      <td>6.739054e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_prevalence</th>\n",
       "      <td>590.0</td>\n",
       "      <td>0.370835</td>\n",
       "      <td>0.100112</td>\n",
       "      <td>0.091189</td>\n",
       "      <td>0.301844</td>\n",
       "      <td>0.379337</td>\n",
       "      <td>0.445031</td>\n",
       "      <td>6.739054e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalence_6lag</th>\n",
       "      <td>590.0</td>\n",
       "      <td>0.378064</td>\n",
       "      <td>0.098194</td>\n",
       "      <td>0.091189</td>\n",
       "      <td>0.315132</td>\n",
       "      <td>0.384463</td>\n",
       "      <td>0.446185</td>\n",
       "      <td>6.480009e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid</th>\n",
       "      <td>677.0</td>\n",
       "      <td>1245.867061</td>\n",
       "      <td>2554.983520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.810000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndvi</th>\n",
       "      <td>629.0</td>\n",
       "      <td>0.264661</td>\n",
       "      <td>0.115720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173333</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.343333</td>\n",
       "      <td>6.100000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipc</th>\n",
       "      <td>600.0</td>\n",
       "      <td>0.132433</td>\n",
       "      <td>0.106511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>5.800000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cropdiv</th>\n",
       "      <td>350.0</td>\n",
       "      <td>5.180000</td>\n",
       "      <td>1.905850</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>603.0</td>\n",
       "      <td>167638.360262</td>\n",
       "      <td>253802.381375</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>64346.000000</td>\n",
       "      <td>99157.000000</td>\n",
       "      <td>180166.419425</td>\n",
       "      <td>2.228463e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>677.0</td>\n",
       "      <td>4.341211</td>\n",
       "      <td>2.982736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increase_numeric</th>\n",
       "      <td>590.0</td>\n",
       "      <td>-0.007229</td>\n",
       "      <td>0.085416</td>\n",
       "      <td>-0.272960</td>\n",
       "      <td>-0.060893</td>\n",
       "      <td>-0.010947</td>\n",
       "      <td>0.041342</td>\n",
       "      <td>3.292577e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>district_encoded</th>\n",
       "      <td>677.0</td>\n",
       "      <td>45.483013</td>\n",
       "      <td>24.319163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>8.600000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count           mean            std           min  \\\n",
       "prevalence        677.0       0.376678       0.097403      0.091189   \n",
       "next_prevalence   590.0       0.370835       0.100112      0.091189   \n",
       "prevalence_6lag   590.0       0.378064       0.098194      0.091189   \n",
       "covid             677.0    1245.867061    2554.983520      0.000000   \n",
       "ndvi              629.0       0.264661       0.115720      0.000000   \n",
       "ipc               600.0       0.132433       0.106511      0.000000   \n",
       "cropdiv           350.0       5.180000       1.905850      2.000000   \n",
       "population        603.0  167638.360262  253802.381375  14000.000000   \n",
       "month             677.0       4.341211       2.982736      1.000000   \n",
       "increase_numeric  590.0      -0.007229       0.085416     -0.272960   \n",
       "district_encoded  677.0      45.483013      24.319163      0.000000   \n",
       "\n",
       "                           25%           50%            75%           max  \n",
       "prevalence            0.312611      0.384560       0.445262  6.739054e-01  \n",
       "next_prevalence       0.301844      0.379337       0.445031  6.739054e-01  \n",
       "prevalence_6lag       0.315132      0.384463       0.446185  6.480009e-01  \n",
       "covid                 0.000000      0.000000       5.000000  7.810000e+03  \n",
       "ndvi                  0.173333      0.235000       0.343333  6.100000e-01  \n",
       "ipc                   0.060000      0.110000       0.170000  5.800000e-01  \n",
       "cropdiv               4.000000      5.000000       6.000000  1.000000e+01  \n",
       "population        64346.000000  99157.000000  180166.419425  2.228463e+06  \n",
       "month                 1.000000      7.000000       7.000000  7.000000e+00  \n",
       "increase_numeric     -0.060893     -0.010947       0.041342  3.292577e-01  \n",
       "district_encoded     25.000000     46.000000      66.000000  8.600000e+01  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here we have 677 data points\n",
    "- the cropdiv has only 350 data points, so if we use directly use dropna (dropping values with NaN), the data points will be below 350 of course because we have NaN values for those crop diversity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop every row with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe on date and reset the index\n",
    "df.sort_values('date', inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop districts with less than 7 observations: 'Burco', 'Saakow', 'Rab Dhuure', 'Baydhaba', 'Afmadow'\n",
    "df.drop(df[df['district'].isin(['Burco', 'Saakow', 'Rab Dhuure', 'Baydhaba', 'Afmadow'])].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of district after droping are -  33\n"
     ]
    }
   ],
   "source": [
    "print(\"Total no of district after droping are - \",len(df['district'].value_counts().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prevalence</th>\n",
       "      <td>231.0</td>\n",
       "      <td>0.382090</td>\n",
       "      <td>0.107469</td>\n",
       "      <td>0.127787</td>\n",
       "      <td>0.307242</td>\n",
       "      <td>0.392600</td>\n",
       "      <td>0.462371</td>\n",
       "      <td>6.480009e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_prevalence</th>\n",
       "      <td>231.0</td>\n",
       "      <td>0.385322</td>\n",
       "      <td>0.106224</td>\n",
       "      <td>0.127787</td>\n",
       "      <td>0.306104</td>\n",
       "      <td>0.406036</td>\n",
       "      <td>0.461843</td>\n",
       "      <td>6.480009e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalence_6lag</th>\n",
       "      <td>231.0</td>\n",
       "      <td>0.390581</td>\n",
       "      <td>0.099032</td>\n",
       "      <td>0.127787</td>\n",
       "      <td>0.343341</td>\n",
       "      <td>0.397141</td>\n",
       "      <td>0.460567</td>\n",
       "      <td>6.480009e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid</th>\n",
       "      <td>231.0</td>\n",
       "      <td>512.571429</td>\n",
       "      <td>1256.220443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.583000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndvi</th>\n",
       "      <td>231.0</td>\n",
       "      <td>0.336782</td>\n",
       "      <td>0.117886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250833</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>6.100000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipc</th>\n",
       "      <td>231.0</td>\n",
       "      <td>0.085758</td>\n",
       "      <td>0.073439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>4.200000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cropdiv</th>\n",
       "      <td>231.0</td>\n",
       "      <td>5.281385</td>\n",
       "      <td>1.914146</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>231.0</td>\n",
       "      <td>157535.571146</td>\n",
       "      <td>175293.997359</td>\n",
       "      <td>20212.000000</td>\n",
       "      <td>69575.061686</td>\n",
       "      <td>101133.000000</td>\n",
       "      <td>183233.500000</td>\n",
       "      <td>1.044086e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>231.0</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>2.975678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increase_numeric</th>\n",
       "      <td>231.0</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.085362</td>\n",
       "      <td>-0.272960</td>\n",
       "      <td>-0.037303</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.051307</td>\n",
       "      <td>2.493279e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>district_encoded</th>\n",
       "      <td>231.0</td>\n",
       "      <td>49.242424</td>\n",
       "      <td>24.942928</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>8.500000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count           mean            std           min  \\\n",
       "prevalence        231.0       0.382090       0.107469      0.127787   \n",
       "next_prevalence   231.0       0.385322       0.106224      0.127787   \n",
       "prevalence_6lag   231.0       0.390581       0.099032      0.127787   \n",
       "covid             231.0     512.571429    1256.220443      0.000000   \n",
       "ndvi              231.0       0.336782       0.117886      0.000000   \n",
       "ipc               231.0       0.085758       0.073439      0.000000   \n",
       "cropdiv           231.0       5.281385       1.914146      2.000000   \n",
       "population        231.0  157535.571146  175293.997359  20212.000000   \n",
       "month             231.0       3.571429       2.975678      1.000000   \n",
       "increase_numeric  231.0       0.003231       0.085362     -0.272960   \n",
       "district_encoded  231.0      49.242424      24.942928      3.000000   \n",
       "\n",
       "                           25%            50%            75%           max  \n",
       "prevalence            0.307242       0.392600       0.462371  6.480009e-01  \n",
       "next_prevalence       0.306104       0.406036       0.461843  6.480009e-01  \n",
       "prevalence_6lag       0.343341       0.397141       0.460567  6.480009e-01  \n",
       "covid                 0.000000       0.000000       5.000000  3.583000e+03  \n",
       "ndvi                  0.250833       0.326667       0.426667  6.100000e-01  \n",
       "ipc                   0.030000       0.080000       0.120000  4.200000e-01  \n",
       "cropdiv               4.000000       5.000000       6.000000  1.000000e+01  \n",
       "population        69575.061686  101133.000000  183233.500000  1.044086e+06  \n",
       "month                 1.000000       1.000000       7.000000  7.000000e+00  \n",
       "increase_numeric     -0.037303       0.004394       0.051307  2.493279e-01  \n",
       "district_encoded     25.000000      55.000000      69.000000  8.500000e+01  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So after dropping we are left with only 231 data points\n",
    "- just because of one cropdiv column (350 values) we removed all the other informations from the data because of **dropna**, which does not make sense, either missing values should be computed or the column should have been dropped, similarly fot ipc classification\n",
    "\n",
    "**NOTE** \n",
    "\n",
    "    - becuase of the above approach we are loosing a lot of information (63% of the data), so even if we make new changes (model/data), it may be possible that changes are not reflected because of this limited amount of data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===========================================================================================\n",
    "### RANDOM FOREST CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Below Explanation is taken from \"baseline_mod.py\" ( Adma & TJ ) )\n",
    "################################################################\n",
    "# How the original model building works\n",
    "\n",
    "################################################################\n",
    "# The first 2 nested for loops will try all the different sizes of random forest\n",
    "# Within the innermost nest for loop (for features in subsets(X.columns)), the RF is\n",
    "# trying every single combination of features that are available in the dataframe.\n",
    "\n",
    "# The cross validation is done by calling X[:99]. In this case, the data is ordered by date, and they know already\n",
    "# that there are 33 unique districts, so they use a multiple of 33 whenever doing cross-validation. So here,\n",
    "# they use rows 1-99 to train (first 3 observations) and the next 33 rows (100-132, the 4th observation) to test.\n",
    "# They do this again for a second CV using the first 4 observations to train, and the 5th to test.\n",
    "# This approach removes the temporality of the data. It essentially creates 2 models: 1 based on the first 3\n",
    "# observations (CV-1), and one on the first 4 observations (CV-2), and then combines those two models as a final.\n",
    "# Since the data is sorted by date before being used, this has some naive temporality as it is using the oldest data\n",
    "# for training and the most recent data for testing. However, there is no differentiation between the relevance of how\n",
    "# far back data goes (e.g. it is not a time series!).\n",
    "# ###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/himanshu/data_challenge/Child-Wasting-Prediction/notebooks/baseline_model_acled.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/himanshu/data_challenge/Child-Wasting-Prediction/notebooks/baseline_model_acled.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m clf \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39mnum_trees, max_depth\u001b[39m=\u001b[39mdepth, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/himanshu/data_challenge/Child-Wasting-Prediction/notebooks/baseline_model_acled.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m#Fit to the training data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/himanshu/data_challenge/Child-Wasting-Prediction/notebooks/baseline_model_acled.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(Xtrain, ytrain)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/himanshu/data_challenge/Child-Wasting-Prediction/notebooks/baseline_model_acled.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m#Make a prediction on the test data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/himanshu/data_challenge/Child-Wasting-Prediction/notebooks/baseline_model_acled.ipynb#X10sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m predictions \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(Xtest)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:465\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[39m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_))\n\u001b[0;32m--> 465\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    466\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    468\u001b[0m ]\n\u001b[1;32m    470\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    477\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs,\n\u001b[1;32m    478\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trees)\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:466\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[39m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_))\n\u001b[1;32m    465\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 466\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_estimator(append\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, random_state\u001b[39m=\u001b[39;49mrandom_state)\n\u001b[1;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    468\u001b[0m ]\n\u001b[1;32m    470\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    477\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs,\n\u001b[1;32m    478\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trees)\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/ensemble/_base.py:163\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_estimator\u001b[39m(\u001b[39mself\u001b[39m, append\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    158\u001b[0m     \u001b[39m\"\"\"Make and configure a copy of the `base_estimator_` attribute.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[39m    Warning: This method should be used to properly instantiate new\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    sub-estimators.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     estimator \u001b[39m=\u001b[39m clone(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_estimator_)\n\u001b[1;32m    164\u001b[0m     estimator\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{p: \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator_params})\n\u001b[1;32m    166\u001b[0m     \u001b[39m# TODO: Remove in v1.2\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# criterion \"mse\" and \"mae\" would cause warnings in every call to\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# DecisionTreeRegressor.fit(..)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/base.py:85\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCannot clone object \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mit does not seem to be a scikit-learn \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mestimator as it does not implement a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mrepr\u001b[39m(estimator), \u001b[39mtype\u001b[39m(estimator))\n\u001b[1;32m     82\u001b[0m             )\n\u001b[1;32m     84\u001b[0m klass \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[0;32m---> 85\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     87\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/base.py:210\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mGet parameters for this estimator.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m--> 210\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_param_names():\n\u001b[1;32m    211\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, key)\n\u001b[1;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/base.py:175\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[1;32m    173\u001b[0m \u001b[39m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m# to represent\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m init_signature \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49msignature(init)\n\u001b[1;32m    176\u001b[0m \u001b[39m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n\u001b[1;32m    177\u001b[0m parameters \u001b[39m=\u001b[39m [\n\u001b[1;32m    178\u001b[0m     p\n\u001b[1;32m    179\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m init_signature\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    180\u001b[0m     \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m p\u001b[39m.\u001b[39mVAR_KEYWORD\n\u001b[1;32m    181\u001b[0m ]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/inspect.py:3247\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msignature\u001b[39m(obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39mglobals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_str\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   3246\u001b[0m     \u001b[39m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3247\u001b[0m     \u001b[39mreturn\u001b[39;00m Signature\u001b[39m.\u001b[39;49mfrom_callable(obj, follow_wrapped\u001b[39m=\u001b[39;49mfollow_wrapped,\n\u001b[1;32m   3248\u001b[0m                                    \u001b[39mglobals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mglobals\u001b[39;49m, \u001b[39mlocals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mlocals\u001b[39;49m, eval_str\u001b[39m=\u001b[39;49meval_str)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/inspect.py:2995\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   2992\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_callable\u001b[39m(\u001b[39mcls\u001b[39m, obj, \u001b[39m*\u001b[39m,\n\u001b[1;32m   2993\u001b[0m                   follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39mglobals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_str\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   2994\u001b[0m     \u001b[39m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2995\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   2996\u001b[0m                                     follow_wrapper_chains\u001b[39m=\u001b[39;49mfollow_wrapped,\n\u001b[1;32m   2997\u001b[0m                                     \u001b[39mglobals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mglobals\u001b[39;49m, \u001b[39mlocals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mlocals\u001b[39;49m, eval_str\u001b[39m=\u001b[39;49meval_str)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/inspect.py:2456\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2451\u001b[0m             \u001b[39mreturn\u001b[39;00m sig\u001b[39m.\u001b[39mreplace(parameters\u001b[39m=\u001b[39mnew_params)\n\u001b[1;32m   2453\u001b[0m \u001b[39mif\u001b[39;00m isfunction(obj) \u001b[39mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2454\u001b[0m     \u001b[39m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2455\u001b[0m     \u001b[39m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2456\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[1;32m   2457\u001b[0m                                     skip_bound_arg\u001b[39m=\u001b[39;49mskip_bound_arg,\n\u001b[1;32m   2458\u001b[0m                                     \u001b[39mglobals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mglobals\u001b[39;49m, \u001b[39mlocals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mlocals\u001b[39;49m, eval_str\u001b[39m=\u001b[39;49meval_str)\n\u001b[1;32m   2460\u001b[0m \u001b[39mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[1;32m   2461\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2462\u001b[0m                                    skip_bound_arg\u001b[39m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml/lib/python3.10/inspect.py:2354\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2350\u001b[0m     parameters\u001b[39m.\u001b[39mappend(Parameter(name, annotation\u001b[39m=\u001b[39mannotation,\n\u001b[1;32m   2351\u001b[0m                                 kind\u001b[39m=\u001b[39m_KEYWORD_ONLY,\n\u001b[1;32m   2352\u001b[0m                                 default\u001b[39m=\u001b[39mdefault))\n\u001b[1;32m   2353\u001b[0m \u001b[39m# **kwargs\u001b[39;00m\n\u001b[0;32m-> 2354\u001b[0m \u001b[39mif\u001b[39;00m func_code\u001b[39m.\u001b[39mco_flags \u001b[39m&\u001b[39m CO_VARKEYWORDS:\n\u001b[1;32m   2355\u001b[0m     index \u001b[39m=\u001b[39m pos_count \u001b[39m+\u001b[39m keyword_only_count\n\u001b[1;32m   2356\u001b[0m     \u001b[39mif\u001b[39;00m func_code\u001b[39m.\u001b[39mco_flags \u001b[39m&\u001b[39m CO_VARARGS:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''------------SECTION RANDOM FOREST CROSS VALIDATION--------------'''\n",
    "#WARNING: this process can take some time, since there are a lot of hyperparameters to investigate. The search space can be manually reduced to speed up the process.\n",
    "\n",
    "#Create empty list to store model scores\n",
    "parameter_scores = []\n",
    "\n",
    "#Define target and explanatory variables\n",
    "X = df.drop(columns = ['increase', 'increase_numeric', 'date', 'district', 'prevalence', 'next_prevalence']) #Note that these columns are dropped, the remaining columns are used as explanatory variables\n",
    "y = df['next_prevalence'].values\n",
    "\n",
    "for num_trees in range(num_trees_min, num_trees_max):\n",
    "    \n",
    "    for depth in range(depth_min, depth_max):\n",
    "        \n",
    "        #Investigate every subset of explanatory variables\n",
    "        for features in subsets(X.columns):\n",
    "        \n",
    "            #First CV split. The 99 refers to the first 3 observations for the 33 districts in the data.\n",
    "            Xtrain = X[:99][features].copy().values\n",
    "            ytrain = y[:99]\n",
    "            Xtest = X[99:132][features].copy().values\n",
    "            ytest = y[99:132]\n",
    "\n",
    "            #Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = RandomForestRegressor(n_estimators=num_trees, max_depth=depth, random_state=0)\n",
    "\n",
    "            #Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            #Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            #Calculate mean absolute error\n",
    "            MAE1 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "\n",
    "            #Second CV split. The 132 refers to the first 4 observations for the 33 districts in the data.\n",
    "            Xtrain = X[:132][features].copy().values\n",
    "            ytrain = y[:132]\n",
    "            Xtest = X[132:165][features].copy().values\n",
    "            ytest = y[132:165]\n",
    "\n",
    "            #Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = RandomForestRegressor(n_estimators=num_trees, max_depth=depth, random_state=0)\n",
    "\n",
    "            #Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            #Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            #Calculate mean absolute error\n",
    "            MAE2 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "            #Calculate the mean MAE over the two folds\n",
    "            mean_MAE = (MAE1 + MAE2)/2\n",
    "\n",
    "            #Store the mean MAE together with the used hyperparameters in list \n",
    "            parameter_scores.append((mean_MAE, num_trees, depth, features))\n",
    "\n",
    "#Sort the models based on score and retrieve the hyperparameters of the best model\n",
    "parameter_scores.sort(key=lambda x: x[0])\n",
    "best_model_score = parameter_scores[0][0]\n",
    "best_model_trees = parameter_scores[0][1]\n",
    "best_model_depth = parameter_scores[0][2]\n",
    "best_model_columns = list(parameter_scores[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for baseline model\n",
    "# print(best_model_score,best_model_trees,best_model_depth,best_model_columns)\n",
    "# # 0.058209814532261865 87 6 ['district_encoded']\n",
    "\n",
    "print(best_model_score,best_model_trees,best_model_depth,best_model_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION FINAL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[best_model_columns].values\n",
    "y = df['next_prevalence'].values\n",
    "\n",
    "#If there is only one explanatory variable, the values need to be reshaped for the model\n",
    "if len(best_model_columns) == 1:\n",
    "\tX = X.reshape(-1, 1)\n",
    "\n",
    "#Peform evaluation on full data\n",
    "Xtrain = X[:165]\n",
    "ytrain = y[:165]\n",
    "Xtest = X[165:]\n",
    "ytest = y[165:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators=best_model_trees, max_depth=best_model_depth, random_state=0)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "predictions = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate MAE\n",
    "MAE = mean_absolute_error(ytest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate boolean values for increase or decrease in prevalence. 0 if next prevalence is smaller than current prevalence, 1 otherwise.\n",
    "increase           = [0 if x<y else 1 for x in df.iloc[165:]['next_prevalence'] for y in df.iloc[165:]['prevalence']]\n",
    "predicted_increase = [0 if x<y else 1 for x in predictions                      for y in df.iloc[165:]['prevalence']]\n",
    "\n",
    "#Calculate accuracy of predicted boolean increase/decrease\n",
    "acc = accuracy_score(increase, predicted_increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of trees: 87\n",
      "max_depth: 6\n",
      "columns: ['district_encoded']\n",
      "0.05600520519191712 0.8512396694214877\n"
     ]
    }
   ],
   "source": [
    "#Print model parameters\n",
    "print('no. of trees: ' + str(best_model_trees) + '\\nmax_depth: ' + str(best_model_depth) + '\\ncolumns: ' + str(best_model_columns))\n",
    "\n",
    "#Print model scores\n",
    "print(MAE, acc)\n",
    "\n",
    "# for the baseline model\n",
    "# no. of trees: 87\n",
    "# max_depth: 6\n",
    "# columns: ['district_encoded']\n",
    "# 0.05600520519191712 0.8512396694214877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW Evaluation method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Evaluation Method ( MAE and Classification Accuracy )\n",
    "\n",
    "**Issues**\n",
    "\n",
    "1. Since it is a regression problem, classifing them as increased or decreased can give wrong estimation regarding the model. Here we are trying to predict the prevelence of wasted children, if the next prevelence increased by 0.0001 and the model preidcted that the prevelence is decreased by 0.0001 then even though the model is vary accurate and the difference between the original and prediction is 0.0002, it will be classified as wrong prediction, which is not correct measure at all.\n",
    "\n",
    "2. Although MAE is a good measure of regression problem but it is unit dependent. So just by looking at the number we don't know if the model is good or bad for example say in a regression problem the target variable price is in millions then the error will be big, while when dealing with small scale output variable say height error will be small.\n",
    "\n",
    "\n",
    "#### Proposed - Adjusted R^2\n",
    "\n",
    "**What is Adjusted R^2 ?**\n",
    "1. Adjusted R squared is a modified version of R square (Here basically we are compare how good is our model with respect to mean, if we have predicted mean always), and it is adjusted for the number of independent variables in the model, and it will always be less than or equal to R². R Squared value always increases with the addition of the independent variables which might lead to the addition of the redundant variables in our model. However, the adjusted R-squared solves this problem. because in R^2 as we add new variable the mean error will be more than predict so 1-(predict/mean) will always increase, so we balance the addition of new independent variable\n",
    "\n",
    "**Why Adjusted R^2**\n",
    "\n",
    "1. As explaind above, we need some regression metric which will be unit independent, so out of different regression metric [SSE, MSE, RMSE, MAE, MAPE, R^2, Adjusted R^2] we chose Adjusted R^2 as it is unit independent (not dependent on target variable, like classification where 80% accuracy is independent of what we are classifying), so provide  easier and correct analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42663037870820064"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate R^2\n",
    "# baseline model R^2 = 0.42663037870820064\n",
    "R2 = abs(r2_score(ytest, predictions))\n",
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = number of samples in test set\n",
    "n = len(ytest)\n",
    "\n",
    "# p = number of independent variables\n",
    "p = len(best_model_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4176714783755163"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjusted R^2 model\n",
    "# baseline model adjusted R^2 = 0.4176714783755163\n",
    "\n",
    "Adjusted_R2 = 1-(1-R2)*(n-1)/(n-p-1)\n",
    "Adjusted_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5188bc372fa413aa2565ae5d28228f50ad7b2c4ebb4a82c5900fd598adbb6408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
